%-----------------------------------
%
\chapter{Feed-Forward Layers}
%
%-----------------------------------


Feed-Forward layers are an essential component of any good transformer!
Empirically, they have been observed to contribute to the expressive power transformer models \cite{geva-etal-2021-transformer}.
Here, we give a theoretical treatment of the feed-forward layer, and show how it can be used to compute a variety of important functions.
Although given enough parameters and a large enough input and hidden dimension, a feed-forward layer can approximate many functions \cite{hornik1989multilayer}, we focus on those that can be expressed exactly.


Much of the expressive power of feed-forward layers comes from their non-linear \emph{activation functions}, of which there are many. We define two popular activation functions. Other notable activation functions include sigmoid, softmax, hyperbolic tangent, Exponential Linear Unit (ELU) and Gated Linear Unit (GLU).

\begin{definition}{Pointwise function application}{}
  For any function $f \colon \R^d \to \R^d$ and any vector $\mathbf{x} = [x_i]_{i=1}^d \in \R^d$, we write
  \begin{equation*}
    f\mleft(\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}\mright) =  \begin{bmatrix} f(x_1) \\ f(x_2) \\ \vdots \\ f(x_d) \end{bmatrix}.
  \end{equation*}
\end{definition}

\begin{definition}{ReLU}{}
    $\ReLU : \R^d\to \R^d$, or Rectified Linear Unit, is a non-linear activation function as defined below \cite{Fukushima1975}.

    \[\ReLU(x) = \max(0,x).\]
\end{definition}

\begin{definition}{GELU}{}
    Gaussian Error Linear Unit (GELU) is a non-linear activation function. $\GELU : \R^d\to \R^d$ is computed as follows, where $\Phi$ is the cumulative distribution function of the standard normal distribution and $\sigma(x) = 1/(1+e^{-x})$ \cite{hendrycks2023}.

    \[\GELU(x) = x\,\Phi(x) \approx \sigma(1.702 x).\]
\end{definition}
\DC{This definition doesn't agree with the one in \cref{sec:ffnn_multiplication}.}

\begin{definition}{Feed-Forward Layer}{hidden text?}
    A feed-forward network is a function $f:\R^d\to \R^d$ that computes
    \[f(\mathbf{x}) = L_2(\ReLU(L_1(\mathbf{x})))\]
    where $L_1: \R^d \to \R^{d_{\text{ff}}}$ and $L_2: \R^{d_{\text{ff}}} \to \R^{d}$ are affine transformations. $L_1$ uses linear transformation $W_1:\R^d\to \R^{d_{\text{ff}}}$ and bias $b_1\in\R^{d_{\text{ff}}}$
    \[L_1(\mathbf{x}) = W_1 \mathbf{x} + b_1\]
    and $L_2$ uses linear transformation $W_2:\R^{d_{\text{ff}}}\to \R^d$ and bias $b_2\in\R^{d}$
    \[L_2(\mathbf{x}) = W_2 \mathbf{x} + b_2.\]
\end{definition}

\section{Continuous Piecewise Linear Functions}

\begin{definition}{continuous piecewise linear}{}
  Let $X \subseteq \R^d$. A function $f \colon X \to \R$ is called \emph{continuous piecewise linear (CPWL)} if there are closed subsets $X_1, \ldots, X_n \subseteq X$ such that $\bigcup_{i\in [n]}  X_i = X$, and for all $i \in [n]$, $\left.f\right|_{X_i}$ is affine.
\end{definition}

For the univariate case ($d=1$), suppose we want to construct a FFNN that computes a CPWL function \(f \colon \mathbb{R} \to \mathbb{R}\). Assume that $f$ has $n \ge 2$ pieces (of which the first and last extend to infinity) and is represented by points \(x_1 < \ldots < x_{n+1}\) and values \(y_1, \ldots, y_{n+1}\) such that \(f(x_k) = y_k\) for \(k = 1, \ldots, n+1\).
To do this, we first define the slopes \(m_1, \ldots, m_n\):
\begin{align*}
  m_k &= \frac{y_{k+1} - y_{k}}{x_{k+1} - x_{k}} \enskip \text{for} \enskip k = 1, \ldots, n.
\end{align*}
Then, \(f\) is computed by the following two-layer neural network:
\begin{align*}
    f(x) &= y_2 - m_1 \, \ReLU(x_2-x) + m_2 \, \ReLU(x-x_2) + \sum_{k=3}^{n-1} (m_k - m_{k-1}) \, \ReLU(x - x_k) \\
    &= \begin{dcases}
        y_1 + m_1 (x - x_1) &x \leq x_2 \\
        y_k + m_{k} (x - x_k) & x_k \leq x \leq x_{k+1} \\
        y_n + m_n (x - x_n) & x \geq x_n.
    \end{dcases}
\end{align*}

For the multivariate case ($f \colon \R^d \to \R$), any CPWL function with $k$ pieces can be computed exactly by a FFNN with $O(\log k)$ layers \citep{arora+:2018}.

\section{Identity Function}\label{sec:ffnn_identity}

This one is very easy: zero out the FFNN itself and let the residual connection do all the work.
\begin{align*}
W_1 = W_2 &= \mathbf{0} \\
b_1 = b_2 &= 0.
\end{align*}

\section{Canceling Residual Connections}\label{sec:ffnn_cancel_residual}

    \begin{tabular}{|p{1.5cm}|p{1.5cm}|}
        \hline
        \rowcolor{orange!20} % Set header color here
        \textbf{Input} & \textbf{Output} \\
        \hline
        $\mathbb{R}^d$ & $\mathbb{R}^d$ \\
        \hline
    \end{tabular}

    We can cancel out residual connections in FFNs. If $f\colon\R^d\to \R^d$ is a FFN with parameters $W_1, b_1, W_2, b_2$, there is a FFN $f'$ with parameters
    \begin{align*}
        W_1'&=\begin{bmatrix}
            W_1\\
            \textbf{I}\\
            -\textbf{I}
        \end{bmatrix}  &\quad b_1'&=\begin{bmatrix}
            b_1\\
            \textbf{0}\\
            \textbf{0}
        \end{bmatrix}\\
        W_2'&=\begin{bmatrix}
            W_2& -\textbf{I} & \textbf{I}
        \end{bmatrix}
        & \quad b_2' &= b_2
    \end{align*}
    so that $f'(\mathbf{x}) + \mathbf{x} = f(\mathbf{x})$, that is, it behaves like $f$ without a residual connection \citep{chiang+:icml2023}.

\section{Min and Max}\label{sec:ffnn_minmax}
    \begin{tabular}{|p{1.5cm}|p{1.5cm}|}
        \hline
        \rowcolor{orange!20} % Set header color here
        \textbf{Input} & \textbf{Output} \\
        \hline
        $\R\times\R$ & $\R$ \\
        \hline
    \end{tabular}


    Recall that $\ReLU(x)=\max(0,x)$. Then  $\min(x,y) = x-\ReLU(x-y)$.
    % $= y-\ReLU(y-x)$. To see this, there are two cases:
    \begin{itemize}
        \item If $x<y$, then $\ReLU(x-y)=0$, so $x-\ReLU(x-y)=x = \min(x,y)$.
        \item If $x\geq y$, then $\ReLU(x-y)=x-y$, so $x-\ReLU(x-y)=x-(x-y)=y = \min(x,y)$.
    \end{itemize}
    Similarly, $\max(x,y) = x+\ReLU(y-x)$. Therefore there exist FFNs to compute the min or max of two real numbers:
    \begin{center}
    \begin{tabular}{c@{\hspace*{4em}}c}
      \begin{tikzpicture}[x=1.5cm,y=1.5cm,baseline=1cm]
        \node (x1) at (0,0) [input,label=below:{$x$}];
        \node (x2) at (1,0) [input,label=below:{$y$}];
        \node (h1) at (-0.5,1) [relu] edge node[near start] {$1$} (x1);
        \node (h2) at (0.5,1) [relu] edge node {$-1$} (x1);
        \node (h3) at (1.5,1) [relu] edge node[near start] {$1$} (x1) edge node[auto=left,near start] {$-1$} (x2);
        \node (y) at (0.5,2) [output,label=above:{$\min(x,y)$}] edge node {$1$} (h1) edge node[auto=left] {$-1$} (h2) edge node[auto=left] {$-1$} (h3);
      \end{tikzpicture} &
      \begin{tikzpicture}[x=1.5cm,y=1.5cm,baseline=1cm]
        \node (x1) at (0,0) [input,label=below:{$x$}];
        \node (x2) at (1,0) [input,label=below:{$y$}];
        \node (h1) at (-0.5,1) [relu] edge node[near start] {$1$} (x1);
        \node (h2) at (0.5,1) [relu] edge node {$-1$} (x1);
        \node (h3) at (1.5,1) [relu] edge node[near start] {$-1$} (x1) edge node[auto=left,near start] {$1$} (x2);
        \node (y) at (0.5,2) [output,label=above:{$\max(x,y)$}] edge node {$1$} (h1) edge node[auto=left] {$-1$} (h2) edge node[auto=left] {$1$} (h3);
      \end{tikzpicture}
    \end{tabular}
    \end{center}


\section{Addition and Subtraction}\label{sec:ffnn_addition}

    \begin{tabular}{|p{1.5cm}|p{1.5cm}|}
        \hline
        \rowcolor{orange!20} % Set header color here
        \textbf{Input} & \textbf{Output} \\
        \hline
        $\R^d$ & $\R$ \\
        \hline
    \end{tabular}

    If you have a tensor of dimensions $\R^{d\times n}$, and you want to add
    $d_1$ and $d_2$ into $d_3$, then use FFN with $W_1:\R^d\to \R^4$ and $W_2:\R^4\to \R^d$

    \begin{equation*}
        W_1=
        \begin{blockarray}{cccccccc}
            & & d_1 & & d_2 & & d_3 & \\
            \begin{block}{c[ccccccc]}
                    1& \cdots & 1 & \cdots & 0 & \cdots & 0 & \cdots \\
                    2& \cdots & -1 & \cdots & 0 & \cdots & 0 & \cdots \\
                    3& \cdots & 0 & \cdots & 1 & \cdots & 0 & \cdots \\
                    4& \cdots & 0 & \cdots & -1 & \cdots & 0 & \cdots \\
            \end{block}
        \end{blockarray}
    \end{equation*}

    Regardless whether the values in $d_1$ and $d_2$ are positive or negative, after applying $\ReLU$ the resulting tensor $\begin{bmatrix}
        \ReLU(A_{d_1,*})\phantom- \\
        \ReLU(-A_{d_1,*})\\
        \ReLU(A_{d_2,*})\phantom- \\
        \ReLU(-A_{d_2,*})\\
    \end{bmatrix}$ will store a copy of $|x|$ in the even dimensions if $x$ was positive, and the even dimension if $x$ was negative. Using this, we can construct $W_2$ to add the correct original values together into $d_3$

    \begin{equation*}
    W_1=
    \begin{blockarray}{ccccc}
        & 1& 2& 3&4 \\
        \begin{block}{c[cccc]}
                & \vdots &\vdots & \vdots & \vdots \\
                d_1 & 0 &\ 0 & 0 & 0 \\
                & \vdots &\vdots & \vdots & \vdots \\
                d_2 & 0 & 0 & 0 & 0 \\
                & \vdots &\vdots & \vdots & \vdots \\
                d_3 & 1  & -1 & 1 & -1 \\
                & \vdots &\vdots & \vdots & \vdots \\
        \end{block}
    \end{blockarray}
    \end{equation*}

    We assume $d_3$ originally held $0$'s, or we can cancel the residual connection as described in \cref{sec:ffnn_cancel_residual}.

\section{Boolean Functions}\label{sec:ffnn_boolean}

    \begin{tabular}{|c|c|}
        \hline
        \rowcolor{orange!20} % Set header color here
        \textbf{Input} & \textbf{Output} \\
        \hline
        $\{0, 1\}$ & $\{0,1\}$ \\
        \hline
    \end{tabular}

    In this section, we show how to compute arbitrary Boolean functions using a single feed-forward network.
    We show the construction for $\text{true}=1, \text{false}=0$ (see \cref{sec:booleans}).
    This is probably the easiest case, but the others are not much more difficult.

    First, the connectives $\land$, $\lor$, $\lnot$ can be computed by FFNNs with $\ReLU$ activations. Conjunction ($\land$) is equivalent to $\min$, disjunction ($\lor$) is equivalent to $\max$, and logical negation ($\lnot$) is just $(1-x)$.

    For an arbitrary Boolean formula $\phi(x_1, \ldots, x_n)$, it can be more involved.
    We treat $\phi$ as a function $\phi \colon \{0,1\}^n \to \{0,1\}$.
    First, convert $\phi$ to \emph{full} disjunctive normal form (DNF), which is a disjunction of clauses, at most one of which can be true for any value of the inputs. The easiest (not necessarily the most efficient) way to do this is to write one clause for every possible combination of inputs that makes $\phi$ true:
    \begin{align}
      \phi(x_1, \ldots, x_n) &= \bigvee_{\substack{\xi_1, \ldots, \xi_n \in \{0, 1\} \\ \phi(\xi_1, \ldots, \xi_n) = 1}} ( x_1 \leftrightarrow \xi_1 \land \cdots \land x_n \leftrightarrow \xi_n ).
    \end{align}
    Note that $x \leftrightarrow 1$ is equivalent to $x$, and $x \leftrightarrow 0$ is equivalent to $\lnot x$.
    For example, the XOR function can be written in full DNF as \[ \text{XOR}(x_1, x_2) = (x_1 \land \neg x_2) \lor (\neg x_1 \land x_2).\]
    As before, logical negation can be computed as $(1-x)$. The $n$-way conjunction can be computed by a slight generalization of the construction for a two-way conjunction. And because at most one clause can be true, the disjunction can be computed simply by addition. So we create a FFNN with $2^n$ hidden units:
    \begin{align}
      \lambda_{i,\xi} &= \begin{cases}
        x_i & \xi = 1 \\
        1-x_i & \xi = 0 
      \end{cases} && i \in [n], \xi \in \{0,1\} && \text{negations} \\
      h_{\xi_1, \ldots, \xi_n} &= \ReLU(\lambda_{1,\xi_1} + \cdots + \lambda_{n,\xi_1} - n + 1) && \xi_1, \ldots, \xi_n \in \{0, 1\} && \text{conjunctions} \\
      y &= \sum_{\substack{\xi_1, \ldots, \xi_n \in \{0,1\} \\ \phi(\xi_1, \ldots, \xi_n) = 1}} h_{\xi_1, \ldots, \xi_n}. && && \text{disjunction}
    \end{align}
    
\section{Conditionals}\label{sec:ffnn_conditional}

    \begin{tabular}{|c|p{1.5cm}|}
        \hline
        \rowcolor{orange!20} % Set header color here
        \textbf{Input} & \textbf{Output} \\
        \hline
        $\{0,1\}\times \R\times\R$ & $\R$ \\
        \hline
    \end{tabular}

    We assume the input representation to a feedforward net (after layer-norm) contains three values: a mask $p \in \{0, 1\}$, a first option $\phi$, and second option $\psi$. The goal is to compute the ``ternary'' expression:
    \begin{equation*}
        \cif{p}{\phi}{\psi} =
        \begin{cases}
            \phi & \textrm{if} \; p \\
            \psi & \textrm{otherwise.}
        \end{cases}
    \end{equation*}

    We adapt a construction used in \citep{merrill2024the} (Theorem 1, arXiv version) for two-layer ReLU feedforward nets.
    Let $\vec 1$ be a vector of ones.
    In the first layer, we define two neurons:
    \begin{align*}
        h_1 &= \ReLU(-p \vec 1 + \vec 1 + \phi) \\
        h_2 &= \ReLU(p \vec 1 - \vec 1 + \psi) .
    \end{align*}
    The second layer then simply computes $h_1 + h_2$, which by construction computes $\cif{p}{\phi}{\psi}$.

    \WM{Can justify this by writing out cases when $p = 0$ or $p = 1$}


\section{Multiplication}
\label{sec:ffnn_multiplication}

By definition~\footnote{\url{https://pytorch.org/docs/stable/generated/torch.nn.GELU.html}}:
\begin{align*}
    \mrm{GeLU}(x) \equiv \frac{x}{2} \parens*{1 + \tanh\parens*{\sqrt{\frac{2}{\pi}} (x + 0.044715x^3)}}
\end{align*}
and so from~\citep[Lemma 4]{akyurek2022learning}:
\begin{align*}
    \sqrt{\frac{\pi}{2}}
    (\mrm{GeLU}(x + y) - \mrm{GeLU}(x) - \mrm{GeLU}(y))
    = xy + \mcal{O}(x^3 + y^3)
\end{align*}
wherein the authors note that
\begin{align*}
    \mrm{GeLU}(x) &= \frac{x}{2} + \sqrt{\frac{2}{\pi}}x^2 + \mcal{O}(x^3) \\
    \mrm{GeLU}(x + y) &- \mrm{GeLU}(x) - \mrm{GeLU}(y) = \sqrt{\frac{2}{\pi}} xy + \mcal{O}(x^3 + y^3)
\end{align*}



